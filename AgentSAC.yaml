default_settings:
  trainer_type: sac
  hyperparameters:
    # Hyperparameters common to PPO and SAC
    batch_size: 1024
    buffer_size: 10240
    learning_rate: 3.0e-4
    learning_rate_schedule: linear

    # SAC-specific hyperparameters
    buffer_init_steps: 0
    tau: 0.005
    steps_per_update: 10.0
    save_replay_buffer: false
    init_entcoef: 0.5
    reward_signal_steps_per_update: 10.0

  # Configuration of the neural network (common to PPO/SAC)
  network_settings:
    vis_encode_type: simple
    normalize: false
    hidden_units: 128
    num_layers: 2

    # LSTM
    memory:
      sequence_length: 64
      memory_size: 256

  # Trainer configurations common to all trainers
  max_steps: 5.0e+5
  time_horizon: 64
  summary_freq: 1000
  keep_checkpoints: 100
  checkpoint_interval: 5.0e+4
  threaded: false
  init_path: null

  # behavior cloning
#  behavioral_cloning:
#    demo_path: Demonstrations\DriveDemo.demo
#    strength: 0.5
#    steps: 150000
#    batch_size: 512
#    num_epoch: 3
#    samples_per_update: 0

  reward_signals:
    # environment reward (default)
    extrinsic:
      strength: 1.0
      gamma: 0.99

    # curiosity module
    # curiosity:
    #   strength: 0.02
    #   gamma: 0.99
    #   encoding_size: 256
    #   learning_rate: 3.0e-4

    # GAIL
    # gail:
    #   strength: 0.01
    #   gamma: 0.99
    #   encoding_size: 128
    #   demo_path: Demonstrations\DriveDemo.demo
    #   learning_rate: 3.0e-4
    #   use_actions: false
    #   use_vail: false

  # self-play
  # self_play:
  #   window: 10
  #   play_against_latest_model_ratio: 0.5
  #   save_steps: 50000
  #   swap_steps: 2000
  #   team_change: 100000
